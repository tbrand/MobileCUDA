Result test PASS
TIME RESULT : 33.944748[sec](MIK)
blockNum  : 48000
threadNum : 1024
size      : 196608000
vector size : 786432000
Result test PASS!
TIME RESULT : 103.420197(TEST SMALL)
[Matrix Multiply Using CUDA] - Starting...
GPU Device 0: "Tesla K20c" with compute capability 3.5

MatrixA(11200,11200), MatrixB(11200,11200)
Computing result using CUDA Kernel...
done
Performance= 162.04 GFlop/s, Time= 69362.297 msec, Size= 2809856000000 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
My RESULT : 105.491493
blockNum  : 48000
threadNum : 1024
size      : 196608000
vector size : 786432000
Result test PASS!
TIME RESULT : 108.352562(MEM SMALL)
blockNum  : 37000
threadNum : 1024
size      : 303104000
vector size : 1212416000
Result test PASS!
TIME RESULT : 109.715195(TEST BIG)
[Matrix Multiply Using CUDA] - Starting...
GPU Device 3: "Tesla K20c" with compute capability 3.5

MatrixA(11200,11200), MatrixB(11200,11200)
Computing result using CUDA Kernel...
done
Performance= 172.40 GFlop/s, Time= 65194.695 msec, Size= 2809856000000 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
My RESULT : 111.346230
[Matrix Multiply Using CUDA] - Starting...
GPU Device 1: "Tesla K20c" with compute capability 3.5

MatrixA(14720,14720), MatrixB(14720,14720)
size A : 826
size B : 826
size C : 4
Computing result using CUDA Kernel...
done
Performance= 146.97 GFlop/s, Time= 43404.727 msec, Size= 6379012096000 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
before calling cudaDeviceReset()
after calling cudaDeviceReset()
My RESULT : 127.281586(matrixMul)
blockNum  : 48000
threadNum : 1024
size      : 196608000
vector size : 786432000
Result test PASS!
TIME RESULT : 130.083542(TEST SMALL)
blockNum  : 48000
threadNum : 1024
size      : 196608000
vector size : 786432000
Result test PASS!
TIME RESULT : 135.697159(TEST SMALL)
blockNum  : 37000
threadNum : 1024
size      : 303104000
vector size : 1212416000
Result test PASS!
TIME RESULT : 162.787994(MEM BIG)
cudaSetDeviceFlags(0)
cudaHostAlloc(0)
cudaHostGetDevicePointer(0)
device address : 0x200000000
cudaMemcpyHostToDevice(0)
cudaMalloc(0) : Address 0x2300200000
cudaMemcpyHostToDevice(0)
cudaMemcpyDeviceToHost(0)
Result test PASS
TIME RESULT : 175.002319[sec](MAP)
blockNum  : 48000
threadNum : 1024
size      : 196608000
vector size : 786432000
Result test PASS!
TIME RESULT : 202.206223(TEST SMALL)
[Matrix Multiply Using CUDA] - Starting...
GPU Device 3: "Tesla K20c" with compute capability 3.5

MatrixA(14720,14720), MatrixB(14720,14720)
size A : 826
size B : 826
size C : 4
Computing result using CUDA Kernel...
done
Performance= 146.97 GFlop/s, Time= 43404.102 msec, Size= 6379012096000 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
before calling cudaDeviceReset()
after calling cudaDeviceReset()
My RESULT : 205.265564(matrixMul)
[Matrix Multiply Using CUDA] - Starting...
GPU Device 3: "Tesla K20c" with compute capability 3.5

MatrixA(14720,14720), MatrixB(14720,14720)
size A : 826
size B : 826
size C : 4
Computing result using CUDA Kernel...
done
Performance= 196.22 GFlop/s, Time= 32508.951 msec, Size= 6379012096000 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
before calling cudaDeviceReset()
after calling cudaDeviceReset()
My RESULT : 209.394150(matrixMul)
[Matrix Multiply Using CUDA] - Starting...
GPU Device 3: "Tesla K20c" with compute capability 3.5

MatrixA(11200,11200), MatrixB(11200,11200)
Computing result using CUDA Kernel...
done
Performance= 128.79 GFlop/s, Time= 87268.234 msec, Size= 2809856000000 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
My RESULT : 212.266220
cudaSetDeviceFlags(0)
cudaHostAlloc(0)
cudaHostGetDevicePointer(0)
device address : 0x200000000
cudaMemcpyHostToDevice(0)
cudaMalloc(0) : Address 0x2300200000
cudaMemcpyHostToDevice(0)
cudaMemcpyDeviceToHost(0)
Result test PASS
TIME RESULT : 219.961426[sec](MAP)
[Matrix Multiply Using CUDA] - Starting...
GPU Device 0: "Tesla K20c" with compute capability 3.5

MatrixA(11200,11200), MatrixB(11200,11200)
Computing result using CUDA Kernel...
done
Performance= 65.43 GFlop/s, Time= 171765.625 msec, Size= 2809856000000 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
My RESULT : 235.825516
[Matrix Multiply Using CUDA] - Starting...
GPU Device 0: "Tesla K20c" with compute capability 3.5

MatrixA(10240,10240), MatrixB(20480,10240)
size A : 400
size B : 800
size C : 4
Computing result using CUDA Kernel...
done
Performance= 220.27 GFlop/s, Time= 58495.438 msec, Size= 4294967296000 Ops, WorkgroupSize= 1024 threads/block
Checking computed result for correctness: Result = PASS

Note: For peak performance, please refer to the matrixMulCUBLAS example.
before calling cudaDeviceReset()
after calling cudaDeviceReset()
My RESULT : 240.584213(matrixMul)
cudaSetDeviceFlags(0)
cudaHostAlloc(0)
cudaHostGetDevicePointer(0)
device address : 0x200000000
cudaMemcpyHostToDevice(0)
cudaMalloc(0) : Address 0x2300200000
cudaMemcpyHostToDevice(0)
cudaMemcpyDeviceToHost(0)
Result test PASS
TIME RESULT : 244.475647[sec](MAP)
Vector SIZE : 976[Mbyte]
cudaMemcpyToSymbol(0)
cudaMemcpyFromSymbol(0)
Result test : PASS
TIME RESULT : 149.501617[sec](DEV MEM)


Result Time : 302[sec]

PROC[0]
	Iden : 5
	Pid  : 658
		TIME  : 151[sec]
		START : 16:31:54
		END   : 16:34:25
PROC[1]
	Iden : 5
	Pid  : 659
		TIME  : 223[sec]
		START : 16:31:54
		END   : 16:35:37
PROC[2]
	Iden : 7
	Pid  : 660
		TIME  : 184[sec]
		START : 16:31:54
		END   : 16:34:58
PROC[3]
	Iden : 1
	Pid  : 661
		TIME  : 226[sec]
		START : 16:31:54
		END   : 16:35:40
PROC[4]
	Iden : 2
	Pid  : 662
		TIME  : 256[sec]
		START : 16:31:54
		END   : 16:36:10
PROC[5]
	Iden : 9
	Pid  : 663
		TIME  : 302[sec]
		START : 16:31:54
		END   : 16:36:56
PROC[6]
	Iden : 5
	Pid  : 664
		TIME  : 124[sec]
		START : 16:31:54
		END   : 16:33:58
PROC[7]
	Iden : 5
	Pid  : 665
		TIME  : 156[sec]
		START : 16:31:54
		END   : 16:34:30
PROC[8]
	Iden : 10
	Pid  : 666
		TIME  : 54[sec]
		START : 16:31:54
		END   : 16:32:48
PROC[9]
	Iden : 2
	Pid  : 667
		TIME  : 132[sec]
		START : 16:31:54
		END   : 16:34:06
PROC[10]
	Iden : 1
	Pid  : 668
		TIME  : 148[sec]
		START : 16:31:54
		END   : 16:34:22
PROC[11]
	Iden : 4
	Pid  : 669
		TIME  : 130[sec]
		START : 16:31:54
		END   : 16:34:04
PROC[12]
	Iden : 1
	Pid  : 670
		TIME  : 230[sec]
		START : 16:31:54
		END   : 16:35:44
PROC[13]
	Iden : 2
	Pid  : 671
		TIME  : 126[sec]
		START : 16:31:54
		END   : 16:34:00
PROC[14]
	Iden : 8
	Pid  : 672
		TIME  : 129[sec]
		START : 16:31:54
		END   : 16:34:03
PROC[15]
	Iden : 11
	Pid  : 673
		TIME  : 253[sec]
		START : 16:31:54
		END   : 16:36:07
PROC[16]
	Iden : 11
	Pid  : 674
		TIME  : 196[sec]
		START : 16:31:54
		END   : 16:35:10
PROC[17]
	Iden : 11
	Pid  : 675
		TIME  : 278[sec]
		START : 16:31:54
		END   : 16:36:32
PROC[18]
	Iden : 0
	Pid  : 676
		TIME  : 274[sec]
		START : 16:31:54
		END   : 16:36:28
PROC[19]
	Iden : 2
	Pid  : 677
		TIME  : 233[sec]
		START : 16:31:54
		END   : 16:35:47
